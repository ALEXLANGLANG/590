{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1 \n",
    "\n",
    "### 1.1\n",
    "True. They are methods that use fundamentally different principles. Weight pruning is to remove unimportant weights while weight quantization is to reduce the number of bits that represent a number.\n",
    "\n",
    "### 1.2\n",
    "False. It is weight sharing (quantization) that takes advantage of look-up table (LUT) to improve the efficency and pruning cannot directly benefit the following Huffman coding process. \n",
    "\n",
    "\n",
    "### 1.3\n",
    "False. It depends. For instance, iterative connection pruning will lead to non-structured sparse weight matrices. It that may not bring much speedup on traditional platforms like GPU, even with specifically designed sparse matrix multiplication algorithm.\n",
    "\n",
    "### 1.4\n",
    "True. Let's see the number of bits we need to represnet all N values is n and we can have $2^n = N$. Then we can get $n =log(N)$ . So the expected average length should be $O(log(N))$\n",
    "\n",
    "### 1.5 \n",
    "True. We use  k bits to represent each weight. And there are N weights so we need $Nk$ bits if we ignore the storage for the centroids.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2 Lab 1\n",
    "\n",
    "### (a)\n",
    "\n",
    "![image info](./1_1.jpg)\n",
    "\n",
    "\n",
    "### (b)\n",
    "\n",
    "I think Std works better than fixed percentage.Becasethe weight are distributed like Gaussian, a fixed ratio to std also infers a steady percentage. And pruning a fixed percentage of weights in each layer may not be effcient as it need a “ranking” process of all the weight parameters\n",
    "\n",
    "### (c)\n",
    "\n",
    "I show a part of std implementation. Please refer to code file directly for more details. \n",
    "![image info](./1_3.jpg)\n",
    "\n",
    "### (d)\n",
    "What do you observe from the sparsity pattern for each layer? E.g., is the sparsity structured or unstructured? Which layers are more likely to be sparse? etc. Give explanations to support your findings.\n",
    "\n",
    "The sparsity is unstructured as we can see all layers are pruned with the smimilar sparsity as the screenshot shows when s  = 0.5.\n",
    "\n",
    "\n",
    "|  s | 0.0  | 0.25  |  0.5 |  1.0 |\n",
    "|---|---|---|---|---|\n",
    "|   accuracy |  0.9169 |  0.9134 |   0.8735 |  0.4453 |\n",
    "|   sparsity| 0.000000 | 0.237423 |   0.436723 |  0.724295 |\n",
    "\n",
    "![image info](./1_4.jpg)\n",
    "\n",
    "### (e)\n",
    "I show a part of percentage implementation. Please refer to code file directly for more details. \n",
    "![image info](./1_5.jpg)\n",
    "### (f)\n",
    "\n",
    "What do you observe from the sparsity pattern for each layer? E.g., is the sparsity structured or unstructured? Which layers are more likely to be sparse? etc. Give explanations to support your findings.\n",
    "\n",
    "The sparsity is unstructured as we can see all layers are pruned with the smimilar sparsity as the screenshot shows when q = 50 %.\n",
    "\n",
    "|  q | 0  | 25  |  50 |  75 |\n",
    "|---|---|---|---|---|\n",
    "|   accuracy |  0.9169 |  0.9134 |   0.8371 |  0.2538|\n",
    "|   sparsity| 0.000084 | 0.250000 |   0.50 |  0.7500 |\n",
    "\n",
    "\n",
    "![image info](./1_6.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3 Lab2\n",
    "\n",
    "### (a)\n",
    "\n",
    "We need to use the mask to make sure the weights pruned are always zeros.\n",
    "Compared to the regular training pipelines, I added the for loop to do multiplication between gradient of eay layers with their mask and delete the scheduler.  \n",
    "![image info](./2_1.jpg)\n",
    "\n",
    "### (b)\n",
    "![image info](./2_1.jpg)\n",
    "### (c)\n",
    "\n",
    "For std pruning, We can see pruning and retraining work very well from the table below. Even for s = 0.5, we recover the test accuracy over 90%. I set learning rate very small because we got close to optimum already and larger learning rate will get us away from the optimum. Besides, we can  notice that some pruning can further improve the performance becuase some redudant weights are removed and the model become simpler and powerful. But we cannot remove too many weights, for s = 1.0, the performance of it is below the accuracy without pruning. we need to do tests to find the most suitable s.\n",
    "\n",
    "|  s | 0.0  | 0.25  |  0.5 |  1.0 |\n",
    "|:-:|:-:|:-:|:-:|:-:|\n",
    "|   retraining settings  |  lr = 0.0001, reg = 1e-2 |  lr=0.0001, reg=1e-2 |   lr=0.0001, reg=1e-2 |  lr=0.0001, reg=1e-2 |\n",
    "|   accuracy before retraining |  0.9169 |  0.9134 |   0.8735 |  0.4453 |\n",
    "|   accuracy after retraining |  0.9165 |  0.9172 |   0.9175 |  0.9091 |\n",
    "|   sparsity before retraining | 0.000000 | 0.237423 |   0.436723 |  0.724295 |\n",
    "|   sparsity after retraining | 0.000000 | 0.237423 |   0.436723 |  0.724295 |\n",
    "\n",
    "\n",
    "### (d)\n",
    "\n",
    "\n",
    "For percentage pruning, We can see pruning and retraining work very well from the table below. Even for s = 50, we recover the test accuracy over 91%. I set learning rate very small because we got close to optimum already and larger learning rate will get us away from the optimum. Besides, we can  notice that some pruning can further improve the performance becuase some redudant weights are removed and the model become simpler and powerful. But we cannot remove too many weights, for s = 75, the performance of it is below than the accuracy without pruning. we need to do tests to find the most suitable s.\n",
    "\n",
    "|  q | 0.0  | 25  |  50 |  75 |\n",
    "|:-:|:-:|:-:|:-:|:-:|\n",
    "|   retraining settings  |  lr = 0.0001, reg = 1e-3|  lr = 0.0001, reg = 1e-3 | lr=0.0001, reg=1e-2 |  lr=0.0001, reg=1e-2 |\n",
    "|   accuracy before retraining |  0.9169 |  0.9134 |   0.8735 |  0.2538 |\n",
    "|   accuracy after retraining |  0.9182 |  0.9183 |   0.9167 |  0.9055 |\n",
    "|   sparsity before retraining | 0.000084 | 0.250000 |   0.500000 |  0.750000 |\n",
    "|   sparsity after retraining | 0.000084 | 0.250000 |   0.500000 |  0.750000 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4 Lab3\n",
    "\n",
    "### (a)\n",
    "![image info](./3_1.jpg)\n",
    "### (b)\n",
    "We can observe that histogram become discrete after doing quantization. For instance, in the figure after quantization with bits =2, we can see there are only 4 bars in weight distribution. \n",
    "\n",
    "#### Weights distribution before quantization\n",
    "![image info](./3_21.jpg)\n",
    "#### Weights distribution after quantization with bits = 2 \n",
    "![image info](./3_22.jpg)\n",
    "### (c)\n",
    "\n",
    "From the table below, we can see that as we decrease the bits to represent each number, the test accuracy will decrease. But we can notice that when bits = 6,5,4, there are no large gap between test accuracy but when we set bits = 3, the test accuracy is below 90%.  This is because the less bits we used to represent numebr, the more loss we will have. For instance, 4 bits can have 16 clusters but 3 bits only have 8 clusters.\n",
    "\n",
    "|  q | 6 | 5  |  4 |  3 |\n",
    "|:-:|:-:|:-:|:-:|:-:|\n",
    "|   accuracy before quantization | 0.9133 |  0.9133 |   0.9133 |  0.9133 |\n",
    "|   accuracy after quantization |  0.9131 |  0.9121 |   0.9080 |  0.8785 |\n",
    "\n",
    "### (d)\n",
    "\n",
    "From the table below, I set bits = 4 and we can see Linear initialization have the best performance with test accuracy over 90% while that of Random and Density-based is below 90%. \n",
    "\n",
    "|  Initialization | Random | Density-based  |  Linear |\n",
    "|:-:|:-:|:-:|:-:|\n",
    "|   accuracy before quantization | 0.9133 |  0.9133 |   0.9133 |\n",
    "|   accuracy after quantization |  0.8971 | 0.8995  |   0.9023 | \n",
    "\n",
    "### (e)\n",
    "\n",
    "The average bit length for the weight parameters:  4.1248654869269785"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5 Lab5\n",
    "I set q = 50 for pruning and bits = 4 for quantization and I got the final accuracy = 0.9023.\n",
    "\n",
    " $ ratio =  \\frac{109660}{274144} \\times \\frac{1}{32} \\times 3.3600863894650645 = 0.04200199911688052$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
