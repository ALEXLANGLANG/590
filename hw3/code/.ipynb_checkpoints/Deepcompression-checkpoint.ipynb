{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "You are asked to complete the following files:\n",
    "* **pruned_layers.py**, which contains the pruning of DNNs to reduce the storage of insignificant weight parameters with 2 methods: pruning by percentage and prune by standard deviation.\n",
    "* **train_util.py**, which includes the training process of DNNs with pruned connections.\n",
    "* **quantize.py**, which applies the quantization (weight sharing) part on the DNN to reduce the storage of weight parameters.\n",
    "* **huffman_coding.py**, which applies the Huffman coding onto the weight of DNNs to further compress the weight size.\n",
    "\n",
    "You are asked to submit the following files:\n",
    "* **net_before_pruning.pt**, which is the set of weight parameters before applying pruning on DNN weight parameters.\n",
    "* **net_after_pruning.pt**, which is the set of weight paramters after applying pruning on DNN weight parameters.\n",
    "* **net_after_quantization.pt**, which is the set of weight parameters after applying quantization (weight sharing) on DNN weight parameters.\n",
    "* **codebook_resnet20.npy**, which is the quantization codebook of each layer after applying quantization (weight sharing).\n",
    "* **huffman_encoding.npy**, which is the encoding map of each item within the quantization codebook in the whole DNN architecture.\n",
    "* **huffman_freq.npy**, which is the frequency map of each item within the quantization codebook in the whole DNN. \n",
    "\n",
    "To ensure fair grading policy, we fix the choice of model to ResNet-20. You may check the implementation in **resnet20.py** for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from resnet20 import ResNetCIFAR\n",
    "from train_util import train, finetune_after_prune, test\n",
    "from quantize import quantize_whole_model\n",
    "from huffman_coding import huffman_coding\n",
    "from summary import summary\n",
    "import torch\n",
    "import numpy as np\n",
    "from prune import prune\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full-precision model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net = ResNetCIFAR(num_layers=20)\n",
    "net = net.to(device)\n",
    "\n",
    "# Uncomment to load pretrained weights\n",
    "# net.load_state_dict(torch.load(\"net_before_pruning.pt\"))\n",
    "# Comment if you have loaded pretrained weights\n",
    "# train(net, epochs=100, batch_size=128, lr= 0.01, reg=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Test Loss=0.2608, Test accuracy=0.9169\n"
     ]
    }
   ],
   "source": [
    "# Load the best weight paramters\n",
    "net.load_state_dict(torch.load(\"net_before_pruning.pt\"))\n",
    "test(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Summary before pruning-----\n",
      "Layer id\tType\t\tParameter\tNon-zero parameter\tSparsity(\\%)\n",
      "1\t\tConvolutional\t864\t\t864\t\t\t0.000000\n",
      "2\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "3\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "4\t\tConvolutional\t4608\t\t4608\t\t\t0.000000\n",
      "5\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "6\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "7\t\tConvolutional\t2304\t\t2304\t\t\t0.000000\n",
      "8\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "9\t\tConvolutional\t512\t\t512\t\t\t0.000000\n",
      "10\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "11\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "12\t\tConvolutional\t2304\t\t2304\t\t\t0.000000\n",
      "13\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "14\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "15\t\tConvolutional\t2304\t\t2304\t\t\t0.000000\n",
      "16\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "17\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "18\t\tConvolutional\t2304\t\t2304\t\t\t0.000000\n",
      "19\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "20\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "21\t\tConvolutional\t2304\t\t2304\t\t\t0.000000\n",
      "22\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "23\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "24\t\tConvolutional\t4608\t\t4608\t\t\t0.000000\n",
      "25\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "26\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "27\t\tConvolutional\t9216\t\t9216\t\t\t0.000000\n",
      "28\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "29\t\tConvolutional\t512\t\t512\t\t\t0.000000\n",
      "30\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "31\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "32\t\tConvolutional\t9216\t\t9216\t\t\t0.000000\n",
      "33\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "34\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "35\t\tConvolutional\t9216\t\t9216\t\t\t0.000000\n",
      "36\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "37\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "38\t\tConvolutional\t9216\t\t9216\t\t\t0.000000\n",
      "39\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "40\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "41\t\tConvolutional\t9216\t\t9216\t\t\t0.000000\n",
      "42\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "43\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "44\t\tConvolutional\t18432\t\t18432\t\t\t0.000000\n",
      "45\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "46\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "47\t\tConvolutional\t36864\t\t36864\t\t\t0.000000\n",
      "48\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "49\t\tConvolutional\t2048\t\t2048\t\t\t0.000000\n",
      "50\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "51\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "52\t\tConvolutional\t36864\t\t36864\t\t\t0.000000\n",
      "53\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "54\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "55\t\tConvolutional\t36864\t\t36864\t\t\t0.000000\n",
      "56\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "57\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "58\t\tConvolutional\t36864\t\t36864\t\t\t0.000000\n",
      "59\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "60\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "61\t\tConvolutional\t36864\t\t36864\t\t\t0.000000\n",
      "62\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "63\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "64\t\tLinear\t\t640\t\t640\t\t\t0.000000\n",
      "Total nonzero parameters: 274144\n",
      "Total parameters: 274144\n",
      "Total sparsity: 0.000000\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"-----Summary before pruning-----\")\n",
    "summary(net)\n",
    "print(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruning & Finetune with pruned connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pruned_layers import PrunedConv,  PruneLinear\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_weights(net):\n",
    "    plt.figure(figsize=(15,35))\n",
    "    count = 1\n",
    "    for name, m in net.named_modules():\n",
    "        if isinstance(m, PruneLinear):\n",
    "            # Get the weight of the module as a NumPy array\n",
    "            weight =  m.linear.weight.data.cpu().numpy()\n",
    "            #Reshape for histogram\n",
    "            weight = weight.reshape(-1)\n",
    "            plt.subplot(8,3,count)\n",
    "            plt.title(\"Weight histogram of layer \"+name)\n",
    "            _ = plt.hist(weight, bins=20)\n",
    "            count  = count +  1\n",
    "\n",
    "        if isinstance(m, PrunedConv): \n",
    "            weight =  m.conv.weight.data.cpu().numpy()\n",
    "            #Reshape for histogram\n",
    "            weight = weight.reshape(-1)\n",
    "            plt.subplot(8,3,count)\n",
    "            plt.title(\"Weight histogram of layer \"+name)\n",
    "            _ = plt.hist(weight, bins=20)\n",
    "            count =  count + 1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Test Loss=1.3079, Test accuracy=0.5941\n"
     ]
    }
   ],
   "source": [
    "# Test accuracy before fine-tuning 'percentage'\n",
    "net.load_state_dict(torch.load(\"net_before_pruning.pt\"))\n",
    "prune(net, method='percentage', q=60, s=0.0)\n",
    "test(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer id\tType\t\tParameter\tNon-zero parameter\tSparsity(\\%)\n",
      "1\t\tConvolutional\t864\t\t346\t\t\t0.599537\n",
      "2\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "3\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "4\t\tConvolutional\t4608\t\t1843\t\t\t0.600043\n",
      "5\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "6\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "7\t\tConvolutional\t2304\t\t922\t\t\t0.599826\n",
      "8\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "9\t\tConvolutional\t512\t\t205\t\t\t0.599609\n",
      "10\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "11\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "12\t\tConvolutional\t2304\t\t922\t\t\t0.599826\n",
      "13\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "14\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "15\t\tConvolutional\t2304\t\t922\t\t\t0.599826\n",
      "16\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "17\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "18\t\tConvolutional\t2304\t\t922\t\t\t0.599826\n",
      "19\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "20\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "21\t\tConvolutional\t2304\t\t922\t\t\t0.599826\n",
      "22\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "23\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "24\t\tConvolutional\t4608\t\t1843\t\t\t0.600043\n",
      "25\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "26\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "27\t\tConvolutional\t9216\t\t3686\t\t\t0.600043\n",
      "28\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "29\t\tConvolutional\t512\t\t205\t\t\t0.599609\n",
      "30\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "31\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "32\t\tConvolutional\t9216\t\t3686\t\t\t0.600043\n",
      "33\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "34\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "35\t\tConvolutional\t9216\t\t3686\t\t\t0.600043\n",
      "36\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "37\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "38\t\tConvolutional\t9216\t\t3686\t\t\t0.600043\n",
      "39\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "40\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "41\t\tConvolutional\t9216\t\t3686\t\t\t0.600043\n",
      "42\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "43\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "44\t\tConvolutional\t18432\t\t7373\t\t\t0.599989\n",
      "45\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "46\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "47\t\tConvolutional\t36864\t\t14746\t\t\t0.599989\n",
      "48\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "49\t\tConvolutional\t2048\t\t819\t\t\t0.600098\n",
      "50\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "51\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "52\t\tConvolutional\t36864\t\t14746\t\t\t0.599989\n",
      "53\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "54\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "55\t\tConvolutional\t36864\t\t14746\t\t\t0.599989\n",
      "56\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "57\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "58\t\tConvolutional\t36864\t\t14746\t\t\t0.599989\n",
      "59\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "60\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "61\t\tConvolutional\t36864\t\t14746\t\t\t0.599989\n",
      "62\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "63\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "64\t\tLinear\t\t640\t\t256\t\t\t0.600000\n",
      "Total nonzero parameters: 109660\n",
      "Total parameters: 274144\n",
      "Total sparsity: 0.599991\n"
     ]
    }
   ],
   "source": [
    "summary(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Epoch: 0\n",
      "[Step=50]\tLoss=0.2724\tacc=0.9259\t2326.7 examples/second\n",
      "[Step=100]\tLoss=0.2601\tacc=0.9296\t2294.7 examples/second\n",
      "[Step=150]\tLoss=0.2531\tacc=0.9324\t2175.0 examples/second\n",
      "[Step=200]\tLoss=0.2481\tacc=0.9336\t2351.5 examples/second\n",
      "[Step=250]\tLoss=0.2478\tacc=0.9343\t2136.1 examples/second\n",
      "[Step=300]\tLoss=0.2460\tacc=0.9342\t2169.3 examples/second\n",
      "[Step=350]\tLoss=0.2439\tacc=0.9350\t2399.9 examples/second\n",
      "Test Loss=0.2967, Test acc=0.9049\n",
      "Saving...\n",
      "\n",
      "Epoch: 1\n",
      "[Step=400]\tLoss=0.2116\tacc=0.9531\t1168.5 examples/second\n",
      "[Step=450]\tLoss=0.2224\tacc=0.9432\t2737.7 examples/second\n",
      "[Step=500]\tLoss=0.2224\tacc=0.9419\t2679.2 examples/second\n",
      "[Step=550]\tLoss=0.2245\tacc=0.9404\t2451.0 examples/second\n",
      "[Step=600]\tLoss=0.2226\tacc=0.9409\t2299.0 examples/second\n",
      "[Step=650]\tLoss=0.2217\tacc=0.9411\t2271.7 examples/second\n",
      "[Step=700]\tLoss=0.2198\tacc=0.9418\t2186.6 examples/second\n",
      "[Step=750]\tLoss=0.2201\tacc=0.9414\t2315.9 examples/second\n",
      "Test Loss=0.2881, Test acc=0.9073\n",
      "Saving...\n",
      "\n",
      "Epoch: 2\n",
      "[Step=800]\tLoss=0.2033\tacc=0.9497\t1088.0 examples/second\n",
      "[Step=850]\tLoss=0.2113\tacc=0.9457\t2568.3 examples/second\n",
      "[Step=900]\tLoss=0.2097\tacc=0.9461\t2678.7 examples/second\n",
      "[Step=950]\tLoss=0.2114\tacc=0.9455\t2675.9 examples/second\n",
      "[Step=1000]\tLoss=0.2137\tacc=0.9444\t2510.6 examples/second\n",
      "[Step=1050]\tLoss=0.2142\tacc=0.9442\t2288.9 examples/second\n",
      "[Step=1100]\tLoss=0.2126\tacc=0.9449\t2321.2 examples/second\n",
      "[Step=1150]\tLoss=0.2139\tacc=0.9439\t2629.6 examples/second\n",
      "Test Loss=0.2868, Test acc=0.9093\n",
      "Saving...\n",
      "\n",
      "Epoch: 3\n",
      "[Step=1200]\tLoss=0.2020\tacc=0.9468\t1105.0 examples/second\n",
      "[Step=1250]\tLoss=0.2027\tacc=0.9465\t2204.8 examples/second\n",
      "[Step=1300]\tLoss=0.2023\tacc=0.9469\t2342.0 examples/second\n",
      "[Step=1350]\tLoss=0.2060\tacc=0.9454\t2450.6 examples/second\n",
      "[Step=1400]\tLoss=0.2065\tacc=0.9454\t2442.1 examples/second\n",
      "[Step=1450]\tLoss=0.2068\tacc=0.9451\t2193.7 examples/second\n",
      "[Step=1500]\tLoss=0.2081\tacc=0.9447\t2384.7 examples/second\n",
      "[Step=1550]\tLoss=0.2073\tacc=0.9449\t2644.2 examples/second\n",
      "Test Loss=0.2801, Test acc=0.9110\n",
      "Saving...\n",
      "\n",
      "Epoch: 4\n",
      "[Step=1600]\tLoss=0.2077\tacc=0.9442\t1106.5 examples/second\n",
      "[Step=1650]\tLoss=0.2074\tacc=0.9438\t2326.2 examples/second\n",
      "[Step=1700]\tLoss=0.2066\tacc=0.9434\t2279.5 examples/second\n",
      "[Step=1750]\tLoss=0.2039\tacc=0.9445\t2436.0 examples/second\n",
      "[Step=1800]\tLoss=0.2018\tacc=0.9448\t2389.9 examples/second\n",
      "[Step=1850]\tLoss=0.2014\tacc=0.9454\t2169.7 examples/second\n",
      "[Step=1900]\tLoss=0.2008\tacc=0.9455\t2151.9 examples/second\n",
      "[Step=1950]\tLoss=0.2022\tacc=0.9447\t2552.8 examples/second\n",
      "Test Loss=0.2773, Test acc=0.9093\n",
      "\n",
      "Epoch: 5\n",
      "[Step=2000]\tLoss=0.1927\tacc=0.9488\t1190.5 examples/second\n",
      "[Step=2050]\tLoss=0.1944\tacc=0.9479\t2583.7 examples/second\n",
      "[Step=2100]\tLoss=0.1922\tacc=0.9486\t2645.4 examples/second\n",
      "[Step=2150]\tLoss=0.1957\tacc=0.9472\t2606.7 examples/second\n",
      "[Step=2200]\tLoss=0.1944\tacc=0.9471\t2326.1 examples/second\n",
      "[Step=2250]\tLoss=0.1953\tacc=0.9470\t2282.0 examples/second\n",
      "[Step=2300]\tLoss=0.1964\tacc=0.9464\t2348.1 examples/second\n",
      "Test Loss=0.2753, Test acc=0.9120\n",
      "Saving...\n",
      "\n",
      "Epoch: 6\n",
      "[Step=2350]\tLoss=0.1779\tacc=0.9531\t1158.9 examples/second\n",
      "[Step=2400]\tLoss=0.1904\tacc=0.9479\t2412.1 examples/second\n",
      "[Step=2450]\tLoss=0.1945\tacc=0.9466\t2435.7 examples/second\n",
      "[Step=2500]\tLoss=0.1964\tacc=0.9467\t2355.7 examples/second\n",
      "[Step=2550]\tLoss=0.1946\tacc=0.9473\t2472.4 examples/second\n",
      "[Step=2600]\tLoss=0.1958\tacc=0.9466\t2373.7 examples/second\n",
      "[Step=2650]\tLoss=0.1960\tacc=0.9465\t2375.2 examples/second\n",
      "[Step=2700]\tLoss=0.1947\tacc=0.9472\t2397.7 examples/second\n",
      "Test Loss=0.2751, Test acc=0.9106\n",
      "\n",
      "Epoch: 7\n",
      "[Step=2750]\tLoss=0.2082\tacc=0.9429\t1181.4 examples/second\n",
      "[Step=2800]\tLoss=0.1938\tacc=0.9461\t2275.0 examples/second\n",
      "[Step=2850]\tLoss=0.1955\tacc=0.9459\t2394.6 examples/second\n",
      "[Step=2900]\tLoss=0.1933\tacc=0.9459\t2301.7 examples/second\n",
      "[Step=2950]\tLoss=0.1943\tacc=0.9454\t2403.8 examples/second\n",
      "[Step=3000]\tLoss=0.1957\tacc=0.9448\t2508.2 examples/second\n",
      "[Step=3050]\tLoss=0.1961\tacc=0.9445\t2474.1 examples/second\n",
      "[Step=3100]\tLoss=0.1950\tacc=0.9450\t2520.3 examples/second\n",
      "Test Loss=0.2750, Test acc=0.9123\n",
      "Saving...\n",
      "\n",
      "Epoch: 8\n",
      "[Step=3150]\tLoss=0.1837\tacc=0.9517\t1109.6 examples/second\n",
      "[Step=3200]\tLoss=0.1837\tacc=0.9508\t2486.2 examples/second\n",
      "[Step=3250]\tLoss=0.1853\tacc=0.9499\t2593.1 examples/second\n",
      "[Step=3300]\tLoss=0.1845\tacc=0.9505\t2578.3 examples/second\n",
      "[Step=3350]\tLoss=0.1849\tacc=0.9500\t2470.5 examples/second\n",
      "[Step=3400]\tLoss=0.1868\tacc=0.9491\t2315.2 examples/second\n",
      "[Step=3450]\tLoss=0.1864\tacc=0.9490\t2512.7 examples/second\n",
      "[Step=3500]\tLoss=0.1861\tacc=0.9491\t2610.9 examples/second\n",
      "Test Loss=0.2760, Test acc=0.9117\n",
      "\n",
      "Epoch: 9\n",
      "[Step=3550]\tLoss=0.1923\tacc=0.9486\t1198.0 examples/second\n",
      "[Step=3600]\tLoss=0.1862\tacc=0.9496\t2479.9 examples/second\n",
      "[Step=3650]\tLoss=0.1837\tacc=0.9506\t2759.4 examples/second\n",
      "[Step=3700]\tLoss=0.1844\tacc=0.9502\t2744.4 examples/second\n",
      "[Step=3750]\tLoss=0.1852\tacc=0.9495\t2367.9 examples/second\n",
      "[Step=3800]\tLoss=0.1855\tacc=0.9495\t2495.9 examples/second\n",
      "[Step=3850]\tLoss=0.1861\tacc=0.9490\t2675.5 examples/second\n",
      "[Step=3900]\tLoss=0.1868\tacc=0.9487\t2843.1 examples/second\n",
      "Test Loss=0.2747, Test acc=0.9098\n",
      "\n",
      "Epoch: 10\n",
      "[Step=3950]\tLoss=0.1777\tacc=0.9506\t1129.3 examples/second\n",
      "[Step=4000]\tLoss=0.1819\tacc=0.9491\t2466.5 examples/second\n",
      "[Step=4050]\tLoss=0.1820\tacc=0.9489\t2406.2 examples/second\n",
      "[Step=4100]\tLoss=0.1816\tacc=0.9489\t2541.3 examples/second\n",
      "[Step=4150]\tLoss=0.1835\tacc=0.9480\t2458.5 examples/second\n",
      "[Step=4200]\tLoss=0.1840\tacc=0.9482\t2416.9 examples/second\n",
      "[Step=4250]\tLoss=0.1856\tacc=0.9481\t2651.1 examples/second\n",
      "[Step=4300]\tLoss=0.1849\tacc=0.9487\t2974.5 examples/second\n",
      "Test Loss=0.2757, Test acc=0.9092\n",
      "\n",
      "Epoch: 11\n",
      "[Step=4350]\tLoss=0.1801\tacc=0.9499\t1120.7 examples/second\n",
      "[Step=4400]\tLoss=0.1781\tacc=0.9517\t2556.5 examples/second\n",
      "[Step=4450]\tLoss=0.1797\tacc=0.9507\t2436.6 examples/second\n",
      "[Step=4500]\tLoss=0.1814\tacc=0.9503\t2357.9 examples/second\n",
      "[Step=4550]\tLoss=0.1803\tacc=0.9507\t2413.8 examples/second\n",
      "[Step=4600]\tLoss=0.1814\tacc=0.9501\t2492.9 examples/second\n",
      "[Step=4650]\tLoss=0.1824\tacc=0.9495\t2470.1 examples/second\n",
      "Test Loss=0.2729, Test acc=0.9119\n",
      "\n",
      "Epoch: 12\n",
      "[Step=4700]\tLoss=0.1682\tacc=0.9521\t1240.9 examples/second\n",
      "[Step=4750]\tLoss=0.1748\tacc=0.9516\t2466.2 examples/second\n",
      "[Step=4800]\tLoss=0.1768\tacc=0.9515\t2378.3 examples/second\n",
      "[Step=4850]\tLoss=0.1752\tacc=0.9515\t2603.2 examples/second\n",
      "[Step=4900]\tLoss=0.1764\tacc=0.9519\t2508.4 examples/second\n",
      "[Step=4950]\tLoss=0.1771\tacc=0.9512\t2555.6 examples/second\n",
      "[Step=5000]\tLoss=0.1771\tacc=0.9510\t2581.1 examples/second\n",
      "[Step=5050]\tLoss=0.1773\tacc=0.9512\t2537.6 examples/second\n",
      "Test Loss=0.2708, Test acc=0.9120\n",
      "\n",
      "Epoch: 13\n",
      "[Step=5100]\tLoss=0.1768\tacc=0.9513\t1220.1 examples/second\n",
      "[Step=5150]\tLoss=0.1785\tacc=0.9496\t2643.6 examples/second\n",
      "[Step=5200]\tLoss=0.1806\tacc=0.9488\t2589.9 examples/second\n",
      "[Step=5250]\tLoss=0.1785\tacc=0.9503\t2563.6 examples/second\n",
      "[Step=5300]\tLoss=0.1770\tacc=0.9506\t2467.0 examples/second\n",
      "[Step=5350]\tLoss=0.1764\tacc=0.9511\t2403.5 examples/second\n",
      "[Step=5400]\tLoss=0.1772\tacc=0.9511\t2424.7 examples/second\n",
      "[Step=5450]\tLoss=0.1780\tacc=0.9506\t2451.2 examples/second\n",
      "Test Loss=0.2712, Test acc=0.9123\n",
      "\n",
      "Epoch: 14\n",
      "[Step=5500]\tLoss=0.1735\tacc=0.9516\t1166.5 examples/second\n",
      "[Step=5550]\tLoss=0.1696\tacc=0.9541\t2589.7 examples/second\n",
      "[Step=5600]\tLoss=0.1708\tacc=0.9533\t2585.3 examples/second\n",
      "[Step=5650]\tLoss=0.1736\tacc=0.9516\t2611.7 examples/second\n",
      "[Step=5700]\tLoss=0.1747\tacc=0.9514\t2625.7 examples/second\n",
      "[Step=5750]\tLoss=0.1747\tacc=0.9515\t2308.1 examples/second\n",
      "[Step=5800]\tLoss=0.1736\tacc=0.9522\t2545.3 examples/second\n",
      "[Step=5850]\tLoss=0.1740\tacc=0.9519\t2468.7 examples/second\n",
      "Test Loss=0.2710, Test acc=0.9136\n",
      "Saving...\n",
      "\n",
      "Epoch: 15\n",
      "[Step=5900]\tLoss=0.1578\tacc=0.9571\t1150.2 examples/second\n",
      "[Step=5950]\tLoss=0.1723\tacc=0.9524\t2583.5 examples/second\n",
      "[Step=6000]\tLoss=0.1734\tacc=0.9508\t2309.0 examples/second\n",
      "[Step=6050]\tLoss=0.1757\tacc=0.9509\t2367.2 examples/second\n",
      "[Step=6100]\tLoss=0.1751\tacc=0.9514\t2452.7 examples/second\n",
      "[Step=6150]\tLoss=0.1749\tacc=0.9513\t2407.2 examples/second\n",
      "[Step=6200]\tLoss=0.1730\tacc=0.9522\t2429.2 examples/second\n",
      "[Step=6250]\tLoss=0.1711\tacc=0.9527\t2673.0 examples/second\n",
      "Test Loss=0.2715, Test acc=0.9134\n",
      "\n",
      "Epoch: 16\n",
      "[Step=6300]\tLoss=0.1731\tacc=0.9540\t1106.3 examples/second\n",
      "[Step=6350]\tLoss=0.1717\tacc=0.9543\t2496.6 examples/second\n",
      "[Step=6400]\tLoss=0.1725\tacc=0.9530\t2479.9 examples/second\n",
      "[Step=6450]\tLoss=0.1713\tacc=0.9534\t2426.1 examples/second\n",
      "[Step=6500]\tLoss=0.1708\tacc=0.9529\t2296.7 examples/second\n",
      "[Step=6550]\tLoss=0.1693\tacc=0.9531\t2426.4 examples/second\n",
      "[Step=6600]\tLoss=0.1708\tacc=0.9524\t2668.5 examples/second\n",
      "Test Loss=0.2710, Test acc=0.9122\n",
      "\n",
      "Epoch: 17\n",
      "[Step=6650]\tLoss=0.1638\tacc=0.9557\t1215.0 examples/second\n",
      "[Step=6700]\tLoss=0.1663\tacc=0.9539\t2513.4 examples/second\n",
      "[Step=6750]\tLoss=0.1661\tacc=0.9539\t2342.7 examples/second\n",
      "[Step=6800]\tLoss=0.1651\tacc=0.9548\t2863.9 examples/second\n",
      "[Step=6850]\tLoss=0.1637\tacc=0.9551\t3000.6 examples/second\n",
      "[Step=6900]\tLoss=0.1635\tacc=0.9552\t3010.0 examples/second\n",
      "[Step=6950]\tLoss=0.1646\tacc=0.9547\t3090.3 examples/second\n",
      "[Step=7000]\tLoss=0.1668\tacc=0.9538\t2826.9 examples/second\n",
      "Test Loss=0.2753, Test acc=0.9121\n",
      "\n",
      "Epoch: 18\n",
      "[Step=7050]\tLoss=0.1690\tacc=0.9512\t1189.7 examples/second\n",
      "[Step=7100]\tLoss=0.1679\tacc=0.9543\t2381.2 examples/second\n",
      "[Step=7150]\tLoss=0.1628\tacc=0.9540\t2143.1 examples/second\n",
      "[Step=7200]\tLoss=0.1629\tacc=0.9552\t2291.2 examples/second\n",
      "[Step=7250]\tLoss=0.1631\tacc=0.9558\t2426.3 examples/second\n",
      "[Step=7300]\tLoss=0.1630\tacc=0.9559\t2376.5 examples/second\n",
      "[Step=7350]\tLoss=0.1629\tacc=0.9560\t2404.3 examples/second\n",
      "[Step=7400]\tLoss=0.1651\tacc=0.9547\t2432.8 examples/second\n",
      "Test Loss=0.2692, Test acc=0.9129\n",
      "\n",
      "Epoch: 19\n",
      "[Step=7450]\tLoss=0.1789\tacc=0.9531\t1147.4 examples/second\n",
      "[Step=7500]\tLoss=0.1618\tacc=0.9572\t2348.2 examples/second\n",
      "[Step=7550]\tLoss=0.1687\tacc=0.9535\t2338.9 examples/second\n",
      "[Step=7600]\tLoss=0.1678\tacc=0.9539\t2235.3 examples/second\n",
      "[Step=7650]\tLoss=0.1682\tacc=0.9532\t2211.8 examples/second\n",
      "[Step=7700]\tLoss=0.1665\tacc=0.9540\t2215.2 examples/second\n",
      "[Step=7750]\tLoss=0.1666\tacc=0.9536\t2233.5 examples/second\n",
      "[Step=7800]\tLoss=0.1657\tacc=0.9541\t2454.5 examples/second\n",
      "Test Loss=0.2761, Test acc=0.9110\n"
     ]
    }
   ],
   "source": [
    "# Uncomment to load pretrained weights\n",
    "# Comment if you have loaded pretrained weights\n",
    "finetune_after_prune(net, epochs=20, batch_size=128, lr=0.0001, reg=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Test Loss=0.2710, Test accuracy=0.9136\n"
     ]
    }
   ],
   "source": [
    "# Load the best weight paramters\n",
    "net.load_state_dict(torch.load(\"net_after_pruning.pt\"))\n",
    "test(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Summary After pruning-----\n",
      "Layer id\tType\t\tParameter\tNon-zero parameter\tSparsity(\\%)\n",
      "1\t\tConvolutional\t864\t\t346\t\t\t0.599537\n",
      "2\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "3\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "4\t\tConvolutional\t4608\t\t1843\t\t\t0.600043\n",
      "5\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "6\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "7\t\tConvolutional\t2304\t\t922\t\t\t0.599826\n",
      "8\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "9\t\tConvolutional\t512\t\t205\t\t\t0.599609\n",
      "10\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "11\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "12\t\tConvolutional\t2304\t\t922\t\t\t0.599826\n",
      "13\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "14\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "15\t\tConvolutional\t2304\t\t922\t\t\t0.599826\n",
      "16\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "17\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "18\t\tConvolutional\t2304\t\t922\t\t\t0.599826\n",
      "19\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "20\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "21\t\tConvolutional\t2304\t\t922\t\t\t0.599826\n",
      "22\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "23\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "24\t\tConvolutional\t4608\t\t1843\t\t\t0.600043\n",
      "25\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "26\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "27\t\tConvolutional\t9216\t\t3686\t\t\t0.600043\n",
      "28\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "29\t\tConvolutional\t512\t\t205\t\t\t0.599609\n",
      "30\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "31\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "32\t\tConvolutional\t9216\t\t3686\t\t\t0.600043\n",
      "33\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "34\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "35\t\tConvolutional\t9216\t\t3686\t\t\t0.600043\n",
      "36\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "37\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "38\t\tConvolutional\t9216\t\t3686\t\t\t0.600043\n",
      "39\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "40\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "41\t\tConvolutional\t9216\t\t3686\t\t\t0.600043\n",
      "42\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "43\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "44\t\tConvolutional\t18432\t\t7373\t\t\t0.599989\n",
      "45\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "46\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "47\t\tConvolutional\t36864\t\t14746\t\t\t0.599989\n",
      "48\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "49\t\tConvolutional\t2048\t\t819\t\t\t0.600098\n",
      "50\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "51\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "52\t\tConvolutional\t36864\t\t14746\t\t\t0.599989\n",
      "53\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "54\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "55\t\tConvolutional\t36864\t\t14746\t\t\t0.599989\n",
      "56\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "57\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "58\t\tConvolutional\t36864\t\t14746\t\t\t0.599989\n",
      "59\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "60\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "61\t\tConvolutional\t36864\t\t14746\t\t\t0.599989\n",
      "62\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n",
      "63\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n",
      "64\t\tLinear\t\t640\t\t256\t\t\t0.600000\n",
      "Total nonzero parameters: 109660\n",
      "Total parameters: 274144\n",
      "Total sparsity: 0.599991\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"-----Summary After pruning-----\")\n",
    "summary(net)\n",
    "print(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Test Loss=0.2710, Test accuracy=0.9136\n",
      "Complete 1 layers quantization...\n",
      "Complete 2 layers quantization...\n",
      "Complete 3 layers quantization...\n",
      "Complete 4 layers quantization...\n",
      "Complete 5 layers quantization...\n",
      "Complete 6 layers quantization...\n",
      "Complete 7 layers quantization...\n",
      "Complete 8 layers quantization...\n",
      "Complete 9 layers quantization...\n",
      "Complete 10 layers quantization...\n",
      "Complete 11 layers quantization...\n",
      "Complete 12 layers quantization...\n",
      "Complete 13 layers quantization...\n",
      "Complete 14 layers quantization...\n",
      "Complete 15 layers quantization...\n",
      "Complete 16 layers quantization...\n",
      "Complete 17 layers quantization...\n",
      "Complete 18 layers quantization...\n",
      "Complete 19 layers quantization...\n",
      "Complete 20 layers quantization...\n",
      "Complete 21 layers quantization...\n",
      "Complete 22 layers quantization...\n",
      "Complete 23 layers quantization...\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.2940, Test accuracy=0.9080\n"
     ]
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(\"net_after_pruning.pt\"))\n",
    "test(net)\n",
    "# plot_weights(net)\n",
    "centers = quantize_whole_model(net, bits=4)\n",
    "# plot_weights(net)\n",
    "np.save(\"codebook_resnet20.npy\", centers)\n",
    "torch.save(net.state_dict(), \"net_after_quantization.pt\")\n",
    "test(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Huffman Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original storage for each parameter: 4.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 3.5723 bits\n",
      "Complete 1 layers for Huffman Coding...\n",
      "Original storage for each parameter: 4.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 3.0760 bits\n",
      "Complete 2 layers for Huffman Coding...\n",
      "Original storage for each parameter: 4.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 3.1920 bits\n",
      "Complete 3 layers for Huffman Coding...\n",
      "Original storage for each parameter: 4.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 3.4927 bits\n",
      "Complete 4 layers for Huffman Coding...\n",
      "Original storage for each parameter: 4.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 3.5195 bits\n",
      "Complete 5 layers for Huffman Coding...\n",
      "Original storage for each parameter: 4.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 3.4306 bits\n",
      "Complete 6 layers for Huffman Coding...\n",
      "Original storage for each parameter: 4.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 3.2755 bits\n",
      "Complete 7 layers for Huffman Coding...\n",
      "Original storage for each parameter: 4.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 3.3189 bits\n",
      "Complete 8 layers for Huffman Coding...\n",
      "Original storage for each parameter: 4.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 3.3652 bits\n",
      "Complete 9 layers for Huffman Coding...\n",
      "Original storage for each parameter: 4.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 3.4154 bits\n",
      "Complete 10 layers for Huffman Coding...\n",
      "Original storage for each parameter: 4.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 3.4341 bits\n",
      "Complete 11 layers for Huffman Coding...\n",
      "Original storage for each parameter: 4.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 3.4178 bits\n",
      "Complete 12 layers for Huffman Coding...\n",
      "Original storage for each parameter: 4.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 3.5534 bits\n",
      "Complete 13 layers for Huffman Coding...\n",
      "Original storage for each parameter: 4.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 3.3836 bits\n",
      "Complete 14 layers for Huffman Coding...\n",
      "Original storage for each parameter: 4.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 3.4278 bits\n",
      "Complete 15 layers for Huffman Coding...\n",
      "Original storage for each parameter: 4.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 3.5014 bits\n",
      "Complete 16 layers for Huffman Coding...\n",
      "Original storage for each parameter: 4.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 3.3692 bits\n",
      "Complete 17 layers for Huffman Coding...\n",
      "Original storage for each parameter: 4.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 3.5702 bits\n",
      "Complete 18 layers for Huffman Coding...\n",
      "Original storage for each parameter: 4.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 3.4291 bits\n",
      "Complete 19 layers for Huffman Coding...\n",
      "Original storage for each parameter: 4.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 3.1933 bits\n",
      "Complete 20 layers for Huffman Coding...\n",
      "Original storage for each parameter: 4.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 3.2948 bits\n",
      "Complete 21 layers for Huffman Coding...\n",
      "Original storage for each parameter: 4.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 3.3714 bits\n",
      "Complete 22 layers for Huffman Coding...\n",
      "Original storage for each parameter: 4.0000 bits\n",
      "Average storage for each parameter after Huffman Coding: 2.8984 bits\n",
      "Complete 23 layers for Huffman Coding...\n",
      "the average bit length for the weight parameters:  3.3600863894650645\n"
     ]
    }
   ],
   "source": [
    "frequency_map, encoding_map = huffman_coding(net, centers)\n",
    "np.save(\"huffman_encoding\", encoding_map)\n",
    "np.save(\"huffman_freq\", frequency_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
